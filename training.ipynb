{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/kaggle/input/url-classification/data.csv',on_bad_lines='skip')\n",
    "\n",
    "# Explore the dataset\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the label column that corresponds bad to 1 and good to 0 for binary classification\n",
    "df['label'] = [1 if x == 'bad' else 0 for x in df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def calculate_entropy(url):\n",
    "    # Count frequency of each character in the URL\n",
    "    freq = {}\n",
    "    for char in url:\n",
    "        freq[char] = freq.get(char, 0) + 1\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = 0\n",
    "    length = len(url)\n",
    "    for count in freq.values():\n",
    "        probability = count / length\n",
    "        entropy -= probability * math.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def tokenize_url(url):\n",
    "    return [ord(char) for char in url]\n",
    "\n",
    "\n",
    "def extract_features(df):\n",
    "    # URL length\n",
    "    df['url_length'] = df['url'].apply(lambda x: len(x))\n",
    "\n",
    "    # Count special characters in URL\n",
    "    df['special_char_count'] = df['url'].apply(lambda x: len(re.findall(r'[?&=]', x)))\n",
    "\n",
    "    # Count number of subdomains\n",
    "    df['subdomain_count'] = df['url'].apply(lambda x: len(x.split('.')) - 2)\n",
    "\n",
    "    # Check if an IP address is present in the URL\n",
    "    df['has_ip'] = df['url'].apply(lambda x: 1 if re.search(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', x) else 0)\n",
    "    \n",
    "    # Calculate entropy of the URL\n",
    "    df['entropy'] = df['url'].apply(calculate_entropy)\n",
    "\n",
    "    # Check if the URL contains a suspicious name\n",
    "    suspicious_names = ['confirm', 'account', 'secure', 'banking', 'secure', 'login', 'signin', 'update', 'password', 'verify','free','game','win','click','prize','cash','money','offer','discount','deal','sale','cheap','best','top','amazing','new','hot','popular','trending','sensational','latest','exclusive','limited','urgent','important','breaking','alert','warning','emergency','crisis','critical','fatal','vital','immediate','important','essential','key','necessary','required','compulsory','mandatory','obligatory','pressing','acute','burning','paramount','preeminent','urgent','top-priority','high-priority','crucial']\n",
    "    df['has_suspicious_name'] = df['url'].apply(lambda x: 1 if any(name in x for name in suspicious_names) else 0)\n",
    "    \n",
    "    # Apply tokenization\n",
    "    df['tokenized_url'] = df['url'].apply(tokenize_url)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = extract_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pad sequences manually\n",
    "def pad_sequences_torch(sequences, max_len):\n",
    "    padded_sequences = torch.zeros((len(sequences), max_len), dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = min(len(seq), max_len)\n",
    "        padded_sequences[i, :seq_len] = torch.tensor(seq[:seq_len], dtype=torch.float32)\n",
    "    return padded_sequences\n",
    "\n",
    "max_len = 75  # Max length of URLs\n",
    "# Reduce the max_len for memory efficiency\n",
    "X_rnn = pad_sequences_torch(df['tokenized_url'], max_len)\n",
    "X_additional = df[['special_char_count', 'subdomain_count', 'has_ip', 'entropy', 'has_suspicious_name']].values\n",
    "\n",
    "# Encode labels\n",
    "y = torch.tensor(df['label'].values, dtype=torch.float32)\n",
    "\n",
    "# Train-test split\n",
    "X_rnn_train, X_rnn_test, X_additional_train, X_additional_test, y_train, y_test = train_test_split(\n",
    "    X_rnn, X_additional, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset that includes both RNN inputs and additional features\n",
    "train_data = TensorDataset(torch.tensor(X_rnn_train), torch.tensor(X_additional_train, dtype=torch.float32), y_train)\n",
    "test_data = TensorDataset(torch.tensor(X_rnn_test), torch.tensor(X_additional_test, dtype=torch.float32), y_test)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 64  # Adjust based on memory constraints\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the RNN model with additional features\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, additional_feature_dim):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim + additional_feature_dim, 128)  # Combine RNN output with additional features\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, rnn_input, additional_features):\n",
    "        h, _ = self.rnn(rnn_input)\n",
    "        h = h[:, -1, :]  # Get the last hidden state\n",
    "        \n",
    "        # Concatenate RNN output and additional features\n",
    "        combined_input = torch.cat((h, additional_features), dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(combined_input))\n",
    "        out = torch.sigmoid(self.fc2(x))\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 1  # Single character (ASCII value)\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Binary classification\n",
    "additional_feature_dim = X_additional_train.shape[1]\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = RNNClassifier(input_dim, hidden_dim, output_dim, additional_feature_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model with batching\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_rnn_batch, X_additional_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reshape the RNN input to add the input dimension (for LSTM)\n",
    "        X_rnn_batch = X_rnn_batch.unsqueeze(-1)  # Add input dimension for RNN\n",
    "        y_batch = y_batch.unsqueeze(-1)  # Reshape labels\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_rnn_batch, X_additional_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluate the model with batching\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_rnn_batch, X_additional_batch, y_batch in test_loader:\n",
    "        X_rnn_batch = X_rnn_batch.unsqueeze(-1)  # Add input dimension for RNN\n",
    "        y_pred = model(X_rnn_batch, X_additional_batch)\n",
    "        y_pred = (y_pred > 0.5).int()\n",
    "        \n",
    "        correct += (y_pred.squeeze() == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
